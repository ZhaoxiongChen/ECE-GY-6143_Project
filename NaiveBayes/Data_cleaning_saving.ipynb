{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Data_cleaning_storing.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"rm-yEG5G7Es0"},"source":["# Data Cleaning and Saving\n","\n","The target of this notebook is to clean the corpus and save the clean, tokenized version as text files.\n","\n","These text files are used in other models. (e.g. Naive Bayes, Support Vector Machine)\n","\n","For detailed training set information, please refer to `Data_processing.ipynb` in RNN section."]},{"cell_type":"markdown","metadata":{"id":"ip8RY9um8XRC"},"source":["## 1. Loading Corpus from Disk"]},{"cell_type":"markdown","metadata":{"id":"zJ4JTPr88Qd_"},"source":["The first thing is to locate the corpus directory in local disk. "]},{"cell_type":"code","metadata":{"id":"d_JmZ2R0bdKX"},"source":["# Corpus directory in local disk\n","dir = 'data/rt-polaritydata'"],"execution_count":42,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lrpdmAOObe2S"},"source":["Find positive and negative samples."]},{"cell_type":"code","metadata":{"id":"Axob4HheswZe"},"source":["import os\n","\n","# Walk through 2 files\n","for rootpath, dirnames, filenames in os.walk(dir):\n","    for filename in filenames:\n","        full_path = rootpath + '/' + filename\n","        if 'neg' in filename:\n","            neg_path = full_path\n","        elif 'pos' in filename:\n","            pos_path = full_path"],"execution_count":43,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"28YVtNQWbiHD"},"source":["Read files."]},{"cell_type":"code","metadata":{"id":"nN2hMY1PbiUV"},"source":["def open_file(path):\n","    with open(path, mode='r', errors='replace') as f:\n","        sentence_list = f.readlines()\n","    return sentence_list\n","\n","# Open positive text\n","pos_text_list = open_file(pos_path)\n","# Open negative text\n","neg_text_list = open_file(neg_path)"],"execution_count":45,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-H9OpzQ37Es2"},"source":["Number of sentences in positive and negative corpus."]},{"cell_type":"code","metadata":{"id":"H4YP-h7z7Es2","outputId":"eb4c56b7-c614-4038-bfa4-344bb418734a"},"source":["print(f'Number of sentences in positive corpus: {len(pos_text_list)}')\n","print(f'Number of sentences in negative corpus: {len(neg_text_list)}')"],"execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of sentences in positive corpus: 5331\nNumber of sentences in negative corpus: 5331\n"]}]},{"cell_type":"markdown","metadata":{"id":"fG_rw4Sb0NhM"},"source":["Text format preview."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3drFObJk0Nyz","executionInfo":{"status":"ok","timestamp":1607895649177,"user_tz":300,"elapsed":265,"user":{"displayName":"ZX Chen","photoUrl":"","userId":"01110524324204802966"}},"outputId":"fdcb9888-eb1c-4a0c-8580-0aeedb614c4e"},"source":["count = 3\n","pos_preview = [pos_text_list[i] for i in range(count)]\n","neg_preview = [neg_text_list[i] for i in range(count)]\n","print('\\nPositive corpus preview:')\n","print(*pos_preview)\n","print('Negative corpus preview:')\n","print(*neg_preview)"],"execution_count":47,"outputs":[{"output_type":"stream","name":"stdout","text":["\nPositive corpus preview:\nthe rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal . \n the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth . \n effective but too-tepid biopic\n\nNegative corpus preview:\nsimplistic , silly and tedious . \n it's so laddish and juvenile , only teenage boys could possibly find it funny . \n exploitative and largely devoid of the depth or sophistication that would make watching such a graphic treatment of the crimes bearable . \n\n"]}]},{"cell_type":"markdown","metadata":{"id":"NTlN4Zel8fQj"},"source":["## 2. Cleaning Corpus"]},{"cell_type":"markdown","metadata":{"id":"P5XkjjxhbtCb"},"source":["Since there are too many punctuations, meaningless words and variations (e.g. tense) in the original corpus, we need to clean the data sets by:\n","* Punctuations elimination;\n","* Stopwords elimination;\n","* Lemmatization.\n","\n","These work could resort to `nltk` package."]},{"cell_type":"code","metadata":{"id":"2wnggQY-UeNC"},"source":["import nltk\n","from typing import List\n","\n","# Download NLTK punctuations, stopwords\n","nltk.download('punkt', quiet=True, raise_on_error=True)\n","nltk.download('stopwords', quiet=True, raise_on_error=True)\n","NLTK_STOP_WORDS = list(set(nltk.corpus.stopwords.words('english')))\n","nltk_porter_stemmer = nltk.stem.PorterStemmer()\n","\n","def preprocess_sentence(sentence: str) -> List[str]:\n","    # Tokenize a sentence to words\n","    tokens = nltk.word_tokenize(sentence)\n","    # Convert all letters to lower-case characters\n","    tokens = (token.lower() for token in tokens)\n","    # Remove non-alphabet characters tokens\n","    tokens = (token for token in tokens if token.isalpha())\n","    # Remove stop words\n","    tokens = (token for token in tokens if token not in NLTK_STOP_WORDS)\n","    # Stem (lemmatize) words\n","    tokens = (nltk_porter_stemmer.stem(token) for token in tokens)\n","    return list(tokens)\n","\n","def preprocess_sentence_list(sentence_list: List[str]) -> List[List[str]]:\n","    # Walk through all sentences in sentence_list\n","    token_list = (preprocess_sentence(sentence) for sentence in sentence_list)\n","    # Remove empty lists\n","    return [tokens for tokens in token_list if tokens]"],"execution_count":48,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z7AFsN2o7Es5"},"source":["Here is the comparison about the same sentence before and after preprocessing."]},{"cell_type":"code","metadata":{"id":"BzSO2fg57Es5","outputId":"13fa9f78-4df3-4606-8f14-124e638d43f4"},"source":["# Test sentences\n","test_text_list = ['I thought this movie was great!', 'He thinks that this movie was miserable!']\n","print('Example sentences before preprocessing:', *test_text_list, sep='\\n')\n","print()\n","test_tokens_list = preprocess_sentence_list(test_text_list)\n","print('Example sentences after preprocessing:', *test_tokens_list, sep='\\n')"],"execution_count":51,"outputs":[{"output_type":"stream","name":"stdout","text":["Example sentences before preprocessing:\nI thought this movie was great!\nHe thinks that this movie was miserable!\n\nExample sentences after preprocessing:\n['thought', 'movi', 'great']\n['think', 'movi', 'miser']\n"]}]},{"cell_type":"markdown","metadata":{"id":"knC7iVOl7Es5"},"source":["Number of sentences after preprocessing.\n","\n","The number will shrink since some of the sentences are meaningless to emotion."]},{"cell_type":"code","metadata":{"id":"BAg_yOVQ7Es6","outputId":"ca5249ff-31e3-4539-c7b2-f95ba21ceb4d"},"source":["pos_token_list = preprocess_sentence_list(pos_text_list)\n","neg_token_list = preprocess_sentence_list(neg_text_list)\n","print(f'Number of sentences in positive corpus after preprocessing: {len(pos_token_list)}')\n","print(f'Number of sentences in negative corpus after preprocessing: {len(neg_token_list)}')"],"execution_count":53,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of sentences in positive corpus after preprocessing: 5327\nNumber of sentences in negative corpus after preprocessing: 5328\n"]}]},{"cell_type":"markdown","metadata":{"id":"w4cEolRrfmMq"},"source":["## 3. Tokenized Clean Corpus Saving"]},{"cell_type":"markdown","metadata":{"id":"Tz8W8Q1J8vCE"},"source":["Save the tokenized clean corpus as text files."]},{"cell_type":"code","metadata":{"id":"XxWSc5fffmAw"},"source":["# Token file names\n","token_filename_pos = 'data/pos_sample_tokenized.txt'\n","token_filename_neg = 'data/neg_sample_tokenized.txt'\n","# Combine 2 token lists\n","all_token_list = pos_token_list + neg_token_list\n","token_filename_all = 'data/all_sample_tokenized.txt'\n","\n","# Make files that contain all processed tokens\n","def save_token(token_list, filename):\n","    sentences = []\n","    with open(filename, 'w') as f:\n","        for tokens in token_list:\n","            sentence = ' '.join(tokens)\n","            sentences.append(sentence)\n","            f.write(sentence + '\\n')\n","\n","save_token(token_list=pos_token_list, filename=token_filename_pos)\n","save_token(token_list=neg_token_list, filename=token_filename_neg)\n","save_token(token_list=all_token_list, filename=token_filename_all)"],"execution_count":55,"outputs":[]}]}