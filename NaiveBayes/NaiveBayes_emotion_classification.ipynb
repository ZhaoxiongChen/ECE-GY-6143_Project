{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NaiveBayes_emotion_classification.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPHQBm1i+ZntJp6WRM53J6h"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"NvhdN4DOsxh3"},"source":["# Naive Bayes model\n","\n","In our project, we expect to use Naive Bayes to train [Cornell Dataset](https://www.cs.cornell.edu/people/pabo/movie-review-data/), as comparsion to the performance of LSTM(RNN) version."]},{"cell_type":"markdown","metadata":{"id":"W2IPwM29UJLD"},"source":["## 1 Loading Data"]},{"source":["Load tokenized clean corpus generated by `Data_cleaning_saving.ipynb`."],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["import os\n","\n","def open_file(path):\n","    with open(path, mode='r', errors='replace') as f:\n","        sentence_list = f.readlines()\n","    return sentence_list\n","\n","pos_dir = 'data/pos_sample_tokenized.txt'\n","neg_dir = 'data/neg_sample_tokenized.txt'\n","all_dir = 'data/all_sample_tokenized.txt'\n","\n","# Open text with positive sentences\n","pos_list = open_file(pos_dir)\n","# Open text with negative sentences\n","neg_list = open_file(neg_dir)\n","# Open text with all sentences\n","all_list = open_file(all_dir)"]},{"source":["Build **bag of words** model for further training."],"cell_type":"markdown","metadata":{}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PlPsKrYcqdSE","executionInfo":{"status":"ok","timestamp":1607895658927,"user_tz":300,"elapsed":5699,"user":{"displayName":"ZX Chen","photoUrl":"","userId":"01110524324204802966"}},"outputId":"b330355e-17a1-408d-e259-8b6e96177aac"},"source":["import numpy as np\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","# Make labels for sentences\n","# 1: positive; 0: negative\n","y = np.hstack((np.ones(len(pos_list)),np.zeros(len(neg_list))))\n","print(f'Shape of label: {y.shape}')\n","\n","# Create vectorizer\n","vectorizer = CountVectorizer(input='content', lowercase=False)\n","\n","# Fit vectorizer with all processed tokens\n","# Transform them into sparse vector X\n","X = vectorizer.fit_transform(all_list).toarray()\n","print(f'Shape of vectorized dataset: {X.shape}')"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of label: (10655,)\n","Shape of vectorized dataset: (10655, 11952)\n"]}]},{"cell_type":"markdown","metadata":{"id":"SkDyHwBLXbQE"},"source":["Split training set and test set.\n","\n","The ratio is same as that in LSTM, which is 80% for training and 20% for testing."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1_IePas5XbDm","executionInfo":{"status":"ok","timestamp":1607895659742,"user_tz":300,"elapsed":5320,"user":{"displayName":"ZX Chen","photoUrl":"","userId":"01110524324204802966"}},"outputId":"a0767e8b-4743-4299-eead-7d8571769348"},"source":["from sklearn.model_selection import train_test_split\n","\n","# Define dataset test split ratio\n","RANDOMNESS_SEED = 42\n","DATASET_TEST_SPLIT_RATIO = 0.2\n","\n","Xtr, Xts, ytr, yts = train_test_split(X, y, \n","                                      test_size=DATASET_TEST_SPLIT_RATIO, \n","                                      random_state=RANDOMNESS_SEED, \n","                                      shuffle=True)\n","\n","print(f'Number of features: {Xtr.shape[1]}')\n","print(f'Number of training texts: {Xtr.shape[0]}')\n","print(f'Number of training texts: {Xts.shape[0]}')"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of features: 11952\nNumber of training texts: 8524\nNumber of training texts: 2131\n"]}]},{"cell_type":"markdown","metadata":{"id":"Ggzl7yHdY5ky"},"source":["## 2 Naive Bayes Implementation"]},{"cell_type":"markdown","metadata":{"id":"IKpJJP3_X4Bz"},"source":["### 2.1 Feature Selection Method 1"]},{"cell_type":"markdown","metadata":{"id":"GeWV03LgCy9L"},"source":["#### 2.1.1 Feature Selection\n","\n","Select 3000 words as the feature based on the frequency it appears in the **overall data set**."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"do1I0Bmi1pXg","executionInfo":{"status":"ok","timestamp":1607895660037,"user_tz":300,"elapsed":4228,"user":{"displayName":"ZX Chen","photoUrl":"","userId":"01110524324204802966"}},"outputId":"bc478286-262f-4b2e-ee01-86ed71030293"},"source":["# Select 3000 words\n","feature_num_1 = 3000\n","\n","# Calculate frequency of each words in the overall dataset\n","frequency = np.sum(X, axis=0)\n","\n","# Find the indices of the most frequent words\n","indices_1 = np.argsort(frequency)[-1:(-1*feature_num_1-1):-1]\n","\n","# Show 20 selected features\n","print(f'First 20 selected features in total of {feature_num_1} features:')\n","j = 0 # List index\n","for i in indices_1[:20]:\n","    j += 1\n","    print(f'%02d. {vectorizer.get_feature_names()[i]}' % j)"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["First 20 selected features in total of 3000 features:\n","01. film\n","02. movi\n","03. like\n","04. one\n","05. make\n","06. stori\n","07. charact\n","08. time\n","09. comedi\n","10. good\n","11. even\n","12. much\n","13. work\n","14. perform\n","15. feel\n","16. way\n","17. get\n","18. littl\n","19. look\n","20. love\n"]}]},{"cell_type":"markdown","metadata":{"id":"vDMoQF2cYIjy"},"source":["#### 2.1.2 Training with Bernoulli Naive Bayes\n","\n","In `classifier_1_Bernoulli`:\n","- 1: 3000 most-frequent words, recorded as `indices_1`\n","- Bernoulli: Bernoulli Navie Bayes"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4pHC-94EYIMc","executionInfo":{"status":"ok","timestamp":1607895661340,"user_tz":300,"elapsed":3906,"user":{"displayName":"ZX Chen","photoUrl":"","userId":"01110524324204802966"}},"outputId":"aacfc3ba-ce7f-4bab-9842-6fdd61813378"},"source":["from sklearn.naive_bayes import BernoulliNB\n","\n","# Training and testing data with only the selected feature\n","Xtr_1 = Xtr[:, indices_1]\n","Xts_1 = Xts[:, indices_1]\n","\n","# Create classifier\n","classifier_1_Bernoulli = BernoulliNB(alpha=1.0, binarize=0.5)\n","\n","# Train the classifier\n","classifier_1_Bernoulli.fit(Xtr_1, ytr)"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BernoulliNB(binarize=0.5)"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"p_5I06k4gsMS"},"source":["Show the performance."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nlTMINU1gtsn","executionInfo":{"status":"ok","timestamp":1607895661341,"user_tz":300,"elapsed":2979,"user":{"displayName":"ZX Chen","photoUrl":"","userId":"01110524324204802966"}},"outputId":"81a2019c-2fcd-4eb4-a79a-610acc8128b6"},"source":["from sklearn.metrics import accuracy_score, precision_score, recall_score\n","\n","def report_performance(feature_description: str, yts, ypred):\n","    print(f'Accuracy, precision and recall score for {feature_description} features:')\n","    print(f'Accuracy:\\t{accuracy_score(yts, ypred)}')\n","    print(f'Precision:\\t{precision_score(yts, ypred)}')\n","    print(f'Recall:\\t\\t{recall_score(yts, ypred)}')\n","\n","# Make prediction\n","yhat_1_Bernoulli = classifier_1_Bernoulli.predict(Xts_1)\n","\n","# Show performance\n","report_performance(f'{feature_num_1} most frequent', yts, yhat_1_Bernoulli)"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy, precision and recall score for 3000 most frequent features:\nAccuracy:\t0.7583294228061943\nPrecision:\t0.7675312199807877\nRecall:\t\t0.7453358208955224\n"]}]},{"cell_type":"markdown","metadata":{"id":"-5JzKLdCZORT"},"source":["#### 2.1.3 Training with Multinomial Naive Bayes"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4Lv5jDbFZOq9","executionInfo":{"status":"ok","timestamp":1607895661342,"user_tz":300,"elapsed":1678,"user":{"displayName":"ZX Chen","photoUrl":"","userId":"01110524324204802966"}},"outputId":"2060be05-0d0d-49b1-bf4c-51accbf8045c"},"source":["from sklearn.naive_bayes import MultinomialNB\n","\n","# Create classifier\n","classifier_1_Multinomial = MultinomialNB(alpha=1.0)\n","\n","# Train the classifier\n","classifier_1_Multinomial.fit(Xtr_1, ytr)"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["MultinomialNB()"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"eSlsDfRLhlz5"},"source":["Show the performance."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XAy-HC2phlz5","executionInfo":{"status":"ok","timestamp":1607895661343,"user_tz":300,"elapsed":756,"user":{"displayName":"ZX Chen","photoUrl":"","userId":"01110524324204802966"}},"outputId":"2de49c58-8e2b-43c1-aa57-94189974a1a4"},"source":["# Make prediction\n","yhat_1_Multinomial = classifier_1_Multinomial.predict(Xts_1)\n","\n","# Show performance\n","report_performance(f'{feature_num_1} most frequent', yts, yhat_1_Multinomial)"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy, precision and recall score for 3000 most frequent features:\nAccuracy:\t0.7573908962928203\nPrecision:\t0.7610536218250236\nRecall:\t\t0.7546641791044776\n"]}]},{"cell_type":"markdown","metadata":{"id":"qqr-oBwYD3Xn"},"source":["### 2.2 Feature Selection Method 2\n","\n","Instead of using term frequency, this method selects feature based on Information Gain (IG)."]},{"cell_type":"markdown","metadata":{"id":"npI23QQyYtw4"},"source":["#### 2.2.1 Information Gain Feature Selection\n","\n","Information gain measures the number of bits of information that the presence/absence of a term contributes to the attitude of review (positive/negative).\n","\n","$$IG(C,X_i) = H(C) - H(C|X_i)$$\n","\n","Where:\n","\n","* $X_i = 0\\ or\\ 1$: random variable that represents the occurance of frequency of term i.\n","\n","    In this project, $X_i=1$ means presence while $X_i=0$ means absence.\n","\n","* $C = +\\ or\\ -$: random variable that determines if a review is positive or negative.\n","\n","    In this project, positive reviews $C=+$ are labeled as 1, while the negative reviews $C=-$ are labeled as 0.\n","\n","* $H(C)$: Inherent uncertainity (entropy) of random variable, measured in bits.\n","\n","* $H(C|X_i)$: Uncertainity (entropy) given feature $X_i$ exists or not.\n","\n","* $IG(C,X_i)$: Information gain, how much information does $X_i$ provide about $C$."]},{"cell_type":"markdown","metadata":{"id":"2EoritGFjoFh"},"source":["##### A. Inherent Uncertainity\n","\n","Calculation of inherent uncertainity $H(C)$:\n","\n","$$H(C) = -\\sum\\limits_{c\\in\\{+,-\\}}P(C=c)log(P(C=c))$$\n","\n","$$=-P(C=+)log_2(P(C=+))-P(C=-)log_2(P(C=-))$$"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cBAKxJ7tY8Rc","executionInfo":{"status":"ok","timestamp":1607895662866,"user_tz":300,"elapsed":418,"user":{"displayName":"ZX Chen","photoUrl":"","userId":"01110524324204802966"}},"outputId":"acea3dc7-9049-4502-ae20-13e10d42f95a"},"source":["# P(pos): probability of positive review in training set\n","p_pos = np.sum(ytr).astype(int) / Xtr.shape[0]\n","\n","# P(neg): probability of negative review in training set\n","p_neg = 1 - p_pos\n","\n","# Inherent uncertainity\n","HC = - p_pos * np.log2(p_pos) - p_neg * np.log2(p_neg)\n","print(f'Inherent Uncertainty H(C) = {HC}')"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Inherent Uncertainty H(C) = 0.9999980541295279\n"]}]},{"cell_type":"markdown","metadata":{"id":"2k7GjNCw8KJd"},"source":["##### B. Conditional Entropy: Uncertainty given Specific Words\n","\n","Calculation of conditional entropy $H(C|X_i)$:\n","\n","$$H(C|X_i) = \\sum\\limits_{x}P(X_i = x)H(C|X=x)$$\n","\n","$$=-\\sum\\limits_{c\\in\\{+,-\\}}\\sum\\limits_{x\\in\\{0,1\\}}P(X_i=x, C=c)log(P(C=c|X_i=x))$$\n","\n","Where the joint probability:\n","\n","$$P(X_i=x, C=c)=P(X_i=x|C=c)P(C=c)$$\n","\n","and the conditional probability:\n","\n","$$P(C=x|X_i=x)=\\frac{P(X_i=x|C=c)P(C=c)}{P(X_i=x)}=\\frac{P(X_i=x, C=c)}{P(X_i=x)}$$\n","\n","\n","P.S. Feature selection based on the IG metric only accounts for the occurrence of (not frequency with which terms appear) in the dataset."]},{"cell_type":"code","metadata":{"id":"V7myVh7kFTaT"},"source":["import copy\n","\n","# Convert matrix from term frequency to binary features\n","def binarization(X):\n","    # Copy the original data\n","    X_b = copy.deepcopy(X)\n","    # Define threshold, upper bound and lower bound\n","    threshold, upper, lower = 0.5, 1, 0\n","    # Binarization process\n","    X_b[X_b > threshold] = upper\n","    X_b[X_b <= threshold] = lower\n","\n","    return X_b\n","\n","# Reform document-term matrix to one-hot code\n","# Xtr_b: binarizied matrix of training set Xtr\n","Xtr_b = binarization(Xtr)"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"39G4ZzD0FTqR"},"source":["For each words X_i, count the times of being selected in positive / negative review.\n","\n","Then calculate $P(X_i=x|C=c)$.\n","\n","P.S.\n","\n","Laplacian Smoothing is a technique that add 1 to the value of numerator and add 2 to the value of denominator.\n","\n","The purpose of Laplacian Smoothing is to eliminate the effect of 0 (i.e. a word does not appear in the given sentence) in likelihood estimation. Otherwise, once there is a missing word, the whole likelihood estimation will obtain the result of 0."]},{"cell_type":"code","metadata":{"id":"arj6HuDMIIPF"},"source":["# Boolean array indicating which sample is a positive/negative review in Xtr\n","pos_samples = ytr.astype(bool)\n","neg_samples = np.invert(pos_samples)\n","\n","# Number of positive and negative reviews in Xtr\n","n_pos = np.sum(ytr).astype(int)\n","n_neg = ytr.shape[0] - n_pos\n","\n","# Num{X_i = 1 | C = +}: X_i present in a positive review\n","n_pre_pos = np.sum(Xtr_b[pos_samples, :], axis=0)\n","# P(X_i = 1 | C = +)\n","p_pre_pos = (n_pre_pos + 1) / (n_pos + 2)  # Laplacian Smoothing\n","\n","# Num{X_i = 1 | C = -}: X_i present in a negative review\n","n_pre_neg = np.sum(Xtr_b[neg_samples, :], axis=0)\n","# P(X_i = 1 | C = -)\n","p_pre_neg = (n_pre_neg + 1) / (n_neg + 2)  # Laplacian Smoothing\n","\n","# Num{X_i = 0 | C = +}: X_i absent in a positive review\n","n_abs_pos = n_pos - n_pre_pos\n","# P(X_i = 0 | C = +)\n","p_abs_pos = (n_abs_pos + 1) / (n_pos + 2)  # Laplacian Smoothing\n","\n","# Num{X_i = 0 | C = -}: X_i absent in a negative review\n","n_abs_neg = n_neg - n_pre_neg\n","# P(X_i = 0 | C = -)\n","p_abs_neg = (n_abs_neg + 1) / (n_neg + 2)  # Laplacian Smoothing"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0gllaC4TRibr"},"source":["Calculate probability of positive/negative reviews $P(C=c)$."]},{"cell_type":"code","metadata":{"id":"QdZefq8n8KdI"},"source":["# P(C = +)\n","p_pos = n_pos / Xtr.shape[0]\n","# P(C = -)\n","p_neg = n_neg / Xtr.shape[0]"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6Su4vr-aKcno"},"source":["Calculating joint probability $P(X_i=x, C=c)=P(X_i=x|C=c)P(C=c)$."]},{"cell_type":"code","metadata":{"id":"K1NbM7_oJAiE"},"source":["# P(X_i = 1, C = +)\n","pj_pre_pos = p_pre_pos * p_pos\n","# P(X_i = 1, C = -)\n","pj_pre_neg = p_pre_neg * p_neg\n","# P(X_i = 0, C = +)\n","pj_abs_pos = p_abs_pos * p_pos\n","# P(X_i = 0, C = -)\n","pj_abs_neg = p_abs_neg * p_neg"],"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"suLB33ELIXtJ"},"source":["Probability of each words being present/absent $P(X_i=x)$."]},{"cell_type":"code","metadata":{"id":"gJ8g7GLkIdfs"},"source":["n_pre = np.sum(Xtr_b, axis=0)\n","\n","# P(X_i = 1)\n","p_pre = (n_pre + 1) / (Xtr.shape[0] + 2) # Laplacian Smoothing\n","\n","# P(X_i = 0)\n","p_abs = 1 - p_pre"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pJa5FCEPKqPr"},"source":["Calculating entropy segments and sum up. "]},{"cell_type":"code","metadata":{"id":"JFgvQ534JBtU"},"source":["H1 = - pj_pre_pos * np.log2(pj_pre_pos / p_pre)\n","H2 = - pj_pre_neg * np.log2(pj_pre_neg / p_pre)\n","H3 = - pj_abs_pos * np.log2(pj_abs_pos / p_abs)\n","H4 = - pj_abs_neg * np.log2(pj_abs_neg / p_abs)\n","\n","HC_conditional = H1 + H2 + H3 + H4"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_N2XcAaO-hNj"},"source":["##### C. Information Gain\n","\n","Calculation of information gain $IG(C,X_i)$:\n","\n","$$IG(C,X_i) = H(C) - H(C|X_i)$$\n","\n","Then store IG values in a text file."]},{"cell_type":"code","metadata":{"id":"tezNsLxa-hcW"},"source":["IG = HC - HC_conditional\n","\n","with open('data/IG.txt', 'w') as ig:\n","    for data in IG:\n","        ig.write(data.astype(str) + '\\n')"],"execution_count":27,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s4JUiw6ND6OM"},"source":["Select 3000 words as the feature based on the information gain (IG) it possesses in **training set**."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fxWahwdpD6io","executionInfo":{"status":"ok","timestamp":1607895672365,"user_tz":300,"elapsed":449,"user":{"displayName":"ZX Chen","photoUrl":"","userId":"01110524324204802966"}},"outputId":"a236cadf-383d-4b45-cdd4-8bb45afa5bf6"},"source":["# Select 3000 words\n","feature_num_2 = 3000\n","\n","# By-default, the np.argsort will follow ASCENDING order\n","# Thus, we need to slice the array with negative step\n","indices_2 = np.argsort(IG)[-1:(-1*feature_num_2-1):-1]\n","\n","# Show 20 selected features\n","print(f'First 20 selected features in total of {feature_num_2} features:')\n","j = 0 # List index\n","for i in indices_2[:20]:\n","    j += 1\n","    print(f'%02d. {vectorizer.get_feature_names()[i]}' % j)"],"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["First 20 selected features in total of 3000 features:\n","01. bad\n","02. bore\n","03. beauti\n","04. dull\n","05. perform\n","06. wast\n","07. joke\n","08. movi\n","09. heart\n","10. move\n","11. best\n","12. human\n","13. cultur\n","14. titl\n","15. film\n","16. examin\n","17. flat\n","18. entertain\n","19. unfunni\n","20. funni\n"]}]},{"cell_type":"markdown","metadata":{"id":"afYaHdsxZi84"},"source":["#### 2.2.2 Training with Bernoulli Naive Bayes\n","\n","In `classifier_2_Bernoulli`:\n","- 2: words with largest IG in descending order, recorded as `indices_2`\n","- Bernoulli: Bernoulli Navie Bayes"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dF-yBlcLZi84","executionInfo":{"status":"ok","timestamp":1607895674302,"user_tz":300,"elapsed":843,"user":{"displayName":"ZX Chen","photoUrl":"","userId":"01110524324204802966"}},"outputId":"0731d523-d0b9-4717-d17e-45c575b8afb4"},"source":["# Training and testing data with only the selected feature\n","Xtr_2 = Xtr[:, indices_2]\n","Xts_2 = Xts[:, indices_2]\n","\n","# Create classifier\n","classifier_2_Bernoulli = BernoulliNB(alpha=1.0, binarize=0.5)\n","\n","# Train the classifier\n","classifier_2_Bernoulli.fit(Xtr_2, ytr)"],"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BernoulliNB(binarize=0.5)"]},"metadata":{},"execution_count":20}]},{"cell_type":"markdown","metadata":{"id":"o-oS7N20L-Wi"},"source":["Show performance."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KmISHSuGL-Wj","executionInfo":{"status":"ok","timestamp":1607895675384,"user_tz":300,"elapsed":286,"user":{"displayName":"ZX Chen","photoUrl":"","userId":"01110524324204802966"}},"outputId":"54c699d3-1cc6-40a6-82e8-e7cbe36d74c7"},"source":["# Make prediction\n","yhat_2_Bernoulli = classifier_2_Bernoulli.predict(Xts_2)\n","\n","# Show performance\n","report_performance(f'{feature_num_2} with most IG', yts, yhat_2_Bernoulli)"],"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy, precision and recall score for 3000 with most IG features:\nAccuracy:\t0.7611450023463163\nPrecision:\t0.7740993184031159\nRecall:\t\t0.7416044776119403\n"]}]},{"cell_type":"markdown","metadata":{"id":"Eg_20JZuZi84"},"source":["#### 2.2.3 Training with Multinomial Naive Bayes"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wCa7IRiMZi84","executionInfo":{"status":"ok","timestamp":1607895676029,"user_tz":300,"elapsed":263,"user":{"displayName":"ZX Chen","photoUrl":"","userId":"01110524324204802966"}},"outputId":"619b3d53-43a4-4871-f667-b4ec8f17cd7d"},"source":["# Create classifier\n","classifier_2_Multinomial = MultinomialNB(alpha=1.0)\n","\n","# Train the classifier\n","classifier_2_Multinomial.fit(Xtr_2, ytr)"],"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["MultinomialNB()"]},"metadata":{},"execution_count":22}]},{"cell_type":"markdown","metadata":{"id":"z7-qKu5ELvqG"},"source":["Show performance."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GPijpK-bLxLd","executionInfo":{"status":"ok","timestamp":1607895677388,"user_tz":300,"elapsed":255,"user":{"displayName":"ZX Chen","photoUrl":"","userId":"01110524324204802966"}},"outputId":"eb57a473-c6ea-4902-d058-b3c6a4fdea8b"},"source":["# Make prediction\n","yhat_2_Multinomial = classifier_2_Multinomial.predict(Xts_2)\n","\n","# Show performance\n","report_performance(f'{feature_num_2} with most IG', yts, yhat_2_Multinomial)"],"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy, precision and recall score for 3000 with most IG features:\nAccuracy:\t0.7620835288596903\nPrecision:\t0.7657572906867357\nRecall:\t\t0.7593283582089553\n"]}]}]}