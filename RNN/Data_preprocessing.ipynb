{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "The training dataset used in this project is the sentence polarity dataset v1.0 at [Cornell Dataset](https://www.cs.cornell.edu/people/pabo/movie-review-data/), which contains 5331 positive and negative reviews. \n",
    "This section is to prepare training set (0.8) and test set (0.2) for LSTM model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the nltk library for dataset cleaning, and vectorize the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import sys\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the necessary library: 'stopwords'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Read the file and Data cleaning\n",
    "It is to read the file and convert 'gb18030' encoding to Unicode encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_file(infile, outfile):\n",
    "    with open(infile, 'r',encoding='gb18030', errors='ignore') as f:\n",
    "        txt = f.read()\n",
    "    with open(outfile, 'w') as f:\n",
    "        f.write(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove punctuation through python regular expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "special_char = re.compile(\"[^A-Za-z0-9 ]+\")\n",
    "def clean_sentence(line):\n",
    "    line = line.lower().replace(\"<br />\",\" \")\n",
    "    return re.sub(special_char, '', line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the NLTK to clean the stopwords\n",
    "def clean_stopwords(words):\n",
    "    stopword = {}.fromkeys([line.rstrip() for line in stopwords.words()])\n",
    "    word_nostop = [w for w in words if w not in stopword]\n",
    "    return word_nostop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the 'clean_stopwords' function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['love', 'us', '', '']"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = ['i','love','us',',','?']\n",
    "w = clean_stopwords(w)\n",
    "w = [clean_sentence(i) for i in w ]\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_file('data/rt-polarity.pos', 'data/pos.txt')\n",
    "decode_file('data/rt-polarity.neg', 'data/neg.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate the wordlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the dataset, extract all the words, and count the number of times each word appears. In order to avoid the interference of low-frequency words and reduce the model parameters, we only keep the first 9999 words and add the low-frequency words as 'None' to the wordlist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive file finished\n",
      "the positive file has 111596 words\n",
      "Negative file finished\n",
      "the negative file has 112407 words\n"
     ]
    }
   ],
   "source": [
    "#generate the wordlist and save to a file\n",
    "word_list = []\n",
    "\n",
    "with open('data/neg.txt', 'r') as f:\n",
    "    f_lines = f.readlines()\n",
    "    for line in f_lines:\n",
    "        words = line.strip().split()\n",
    "        words = [clean_sentence(i) for i in words ]\n",
    "        word_list.extend(words)\n",
    "num_pos = len(word_list)\n",
    "print(\"Positive file finished\")\n",
    "print('the positive file has %d words'%num_pos)\n",
    "with open('data/pos.txt', 'r') as f:\n",
    "    f_lines = f.readlines()\n",
    "    for line in f_lines:\n",
    "        words = line.strip().split()\n",
    "        words = [clean_sentence(i) for i in words ]\n",
    "        word_list.extend(words)\n",
    "num_neg = len(word_list)-num_pos\n",
    "print(\"Negative file finished\")\n",
    "print('the negative file has %d words'%num_neg)\n",
    "\n",
    "word_list = clean_stopwords(word_list)\n",
    "counter = collections.Counter(word_list)\n",
    "\n",
    "sorted_words = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    "# the most frequency 10000 words\n",
    "word_list = [word[0] for word in sorted_words]\n",
    "\n",
    "word_list = ['<None>'] + word_list[:10000 - 1]\n",
    "# save to the file\n",
    "with open('data/vocab.txt', 'w') as f:\n",
    "    for word in word_list:\n",
    "        f.write(word + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Use wordlist to convert movie reviews into word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the word index in the wordlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_id_by_word(word, word2id):\n",
    "    if word in word2id:\n",
    "        return word2id[word]\n",
    "    else:\n",
    "        return word2id['<None>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/vocab.txt', 'r') as f:\n",
    "    vocab_list = f.read().strip().split('\\n')\n",
    "word2id = dict(zip(vocab_list, range(len(vocab_list))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the sentence to a vec. Because the length of different movie reviews is different, we choose the longest movie review and fill other film reviews with '0' value. Then convert it into a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the sentence to a vec\n",
    "vec = []\n",
    "with open('data/pos.txt', 'r') as f:\n",
    "    f_lines = f.readlines()\n",
    "    for line in f_lines:\n",
    "        tmp_vec = [str(get_id_by_word(word, word2id)) for word in line.strip().split()]\n",
    "        vec.append(tmp_vec)\n",
    "\n",
    "with open('data/pos.vec', 'w') as f:\n",
    "    for tmp_vec in vec:\n",
    "        f.write(' '.join(tmp_vec) + '\\n')\n",
    "        \n",
    "vec = []\n",
    "with open('data/neg.txt', 'r') as f:\n",
    "    f_lines = f.readlines()\n",
    "    for line in f_lines:\n",
    "        tmp_vec = [str(get_id_by_word(word, word2id)) for word in line.strip().split()]\n",
    "        vec.append(tmp_vec)\n",
    "\n",
    "with open('data/neg.vec', 'w') as f:\n",
    "    for tmp_vec in vec:\n",
    "        f.write(' '.join(tmp_vec) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate training set and test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to achieve better training results, we randomly shuffle the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_data(x, y, path):\n",
    "    \n",
    "    maxlen = max(map(len, x))\n",
    "    data = np.zeros([len(x), maxlen], dtype=np.int32)\n",
    "    for row in range(len(x)):\n",
    "        data[row, :len(x[row])] = x[row]\n",
    "    label = np.array(y)\n",
    "    # shuffle the data\n",
    "    state = np.random.get_state()\n",
    "    np.random.shuffle(data)\n",
    "    np.random.set_state(state)\n",
    "    np.random.shuffle(label)\n",
    "    # save the X and y \n",
    "    np.save(path + '_data', data)\n",
    "    np.save(path + '_labels', label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide the dataset into train set(80%) and test set(20%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[], []]\n",
    "labels = [[], []]\n",
    "rate = np.array([0.8, 0.2])\n",
    "cumsum_rate = np.cumsum(rate)\n",
    "with open('data/pos.vec', 'r') as f:\n",
    "    f_lines = f.readlines()\n",
    "    for line in f_lines:\n",
    "        tmp_data = [int(word) for word in line.strip().split()]\n",
    "        tmp_label = [1, ]\n",
    "        index = int(np.searchsorted(cumsum_rate, np.random.rand(1) * 1.0))\n",
    "        data[index].append(tmp_data)\n",
    "        labels[index].append(tmp_label)\n",
    "with open('data/neg.vec', 'r') as f:\n",
    "    f_lines = f.readlines()\n",
    "    for line in f_lines:\n",
    "        tmp_data = [int(word) for word in line.strip().split()]\n",
    "        tmp_label = [0, ]\n",
    "        index = int(np.searchsorted(cumsum_rate, np.random.rand(1) * 1.0))\n",
    "        data[index].append(tmp_data)\n",
    "        labels[index].append(tmp_label)\n",
    "\n",
    "shuffle_data(data[0], labels[0], 'data/train')\n",
    "shuffle_data(data[1], labels[1], 'data/test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
